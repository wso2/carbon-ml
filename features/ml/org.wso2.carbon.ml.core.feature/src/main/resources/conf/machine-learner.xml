<?xml version="1.0" encoding="UTF-8"?>
<MachineLearner>
	<!-- JNDI name of the data source to be used by the Machine Learner. This 
		data source should be defined in the master-datasources.xml file in conf/datasources 
		directory. -->
	<Database>jdbc/WSO2ML_DB</Database>

	<!-- Configurations to calculate summary statistics from a dataset. histogramBins 
		: Number of intervals in a histogram. categoricalThreshold : The maximum 
		number of categories that can have in a non-string categorical feature. If 
		exeeds, the feature will be treated as a numerical feature. sampleSize : 
		Size of the dataset sample which will be used for summary statistics calculation 
		and visualization. -->
	<SummaryStatisticsSettings>
		<HistogramBins>20</HistogramBins>
		<CategoricalThreshold>10</CategoricalThreshold>
		<SampleSize>10000</SampleSize>
	</SummaryStatisticsSettings>

	<!-- -->
	<Properties>
		<Property name="ml.thread.pool.size" value="100" />
		<!-- default value of the target.home property is {carbon.home}/datasets -->
		<!-- Property name="target.home" value="/tmp" / -->
		<Property name="file.in"
			value="org.wso2.carbon.ml.core.impl.FileInputAdapter" />
		<Property name="file.out"
			value="org.wso2.carbon.ml.core.impl.FileOutputAdapter" />
		<Property name="hdfs.in"
			value="org.wso2.carbon.ml.core.impl.HdfsInputAdapter" />
		<Property name="hdfs.out"
			value="org.wso2.carbon.ml.core.impl.HdfsOutputAdapter" />
		<Property name="das.in" value="org.wso2.carbon.ml.core.impl.BAMInputAdapter" />
		<Property name="registry.in"
			value="org.wso2.carbon.ml.core.impl.RegistryInputAdapter" />
		<Property name="registry.out"
			value="org.wso2.carbon.ml.core.impl.RegistryOutputAdapter" />
	</Properties>

	<!-- Email address to which the model building status mails are sent. -->
	<EmailNotificationEndpoint></EmailNotificationEndpoint>

	<!-- Registry location where ML models are published. -->
	<ModelRegistryLocation>ml</ModelRegistryLocation>

	<!-- Optional element. This element should point to a HDFS location where 
		ML is allowed to store files. -->
	<HdfsURL>hdfs://localhost:9000</HdfsURL>

	<!-- Storage Details for built ML Models. StorageType can be either "file" 
		or "hdfs". Default storage type is file and location is {carbon.home}/models -->
	<!-- ModelStorage> <StorageType>file</StorageType> <StorageDirectory>/tmp</StorageDirectory> 
		</ModelStorage -->

	<Algorithms>
		<Algorithm>
			<Name>LINEAR_REGRESSION</Name>
			<Type>Numerical_Prediction</Type>
			<Interpretability>5</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Iterations</Name>
				<Value>100</Value>
				<Description>Number of times optimizer runs before completing the
					optimization process
				</Description>
			</Parameters>
			<Parameters>
				<Name>Learning_Rate</Name>
				<Value>0.001</Value>
				<Description>Step size of the optimization algorithm</Description>
			</Parameters>
			<Parameters>
				<Name>SGD_Data_Fraction</Name>
				<Value>1</Value>
				<Description>Fraction of the training dataset used per iteration in
					the optimization algorithm
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>RIDGE_REGRESSION</Name>
			<Type>Numerical_Prediction</Type>
			<Interpretability>5</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Iterations</Name>
				<Value>100</Value>
				<Description>Number of times optimizer runs before completing the
					optimization process
				</Description>
			</Parameters>
			<Parameters>
				<Name>Learning_Rate</Name>
				<Value>0.001</Value>
				<Description>Step size of the optimization algorithm</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Parameter</Name>
				<Value>0.001</Value>
				<Description>The regularization parameter reduces overfitting, which
					reduces the variance of your estimated regression parameters;
					however, it does this at the expense of adding bias to your
					estimate. Increasing lambda results in less overfitting but also
					greater bias.</Description>
			</Parameters>
			<Parameters>
				<Name>SGD_Data_Fraction</Name>
				<Value>1</Value>
				<Description>Fraction of the training dataset use per iteration in
					the optimization algorithm
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>LASSO_REGRESSION</Name>
			<Type>Numerical_Prediction</Type>
			<Interpretability>5</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Iterations</Name>
				<Value>100</Value>
				<Description>Number of times optimizer runs before completing the
					optimization process
				</Description>
			</Parameters>
			<Parameters>
				<Name>Learning_Rate</Name>
				<Value>0.001</Value>
				<Description>Step size of the optimization algorithm</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Parameter</Name>
				<Value>0.001</Value>
				<Description>Regularization parameter controls the model complexity
					and hence, helps to control model overfitting.
				</Description>
			</Parameters>
			<Parameters>
				<Name>SGD_Data_Fraction</Name>
				<Value>1</Value>
				<Description>Fraction of the training dataset use per iteration in
					the optimization algorithm
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>LOGISTIC_REGRESSION</Name>
			<Type>Classification</Type>
			<Interpretability>3</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Iterations</Name>
				<Value>100</Value>
				<Description>Number of times optimizer runs before completing the
					optimization process
				</Description>
			</Parameters>
			<Parameters>
				<Name>Learning_Rate</Name>
				<Value>0.1</Value>
				<Description>Step size of the optimization algorithm</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Type</Name>
				<Value>L2</Value>
				<Description>Type of the regularization, wso2 machine learner
					supports L2 and L1 regularizations. Therefore, this parameter
					should be either L1 or L2.
				</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Parameter</Name>
				<Value>0.001</Value>
				<Description>Regularization parameter controls the model complexity
					and hence, helps to control model overfitting
				</Description>
			</Parameters>
			<Parameters>
				<Name>SGD_Data_Fraction</Name>
				<Value>1</Value>
				<Description>Fraction of the training dataset use in a single
					iteration of the optimization algorithm
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>LOGISTIC_REGRESSION_LBFGS</Name>
			<Type>Classification</Type>
			<Interpretability>3</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Reg_Type</Name>
				<Value>L2</Value>
				<Description>Type of the regularization, wso2 machine learner
					supports L2 and L1 regularizations. Therefore, this parameter
					should be either L1 or L2.
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>SVM</Name>
			<Type>Classification</Type>
			<Interpretability>1</Interpretability>
			<Scalability>1</Scalability>
			<Multicollinearity>5</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Learning_Rate</Name>
				<Value>0.001</Value>
				<Description>Step size of the optimization algorithm</Description>
			</Parameters>
			<Parameters>
				<Name>Iterations</Name>
				<Value>100</Value>
				<Description>Number of times optimizer runs before completing the
					optimization process
				</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Type</Name>
				<Value>L1</Value>
				<Description>Type of the regularization, wso2 machine learner
					supports L2 and L1 regularizations. Therefore, this parameter
					should be either L1 or L2.
				</Description>
			</Parameters>
			<Parameters>
				<Name>Reg_Parameter</Name>
				<Value>0.001</Value>
				<Description>Regularization parameter controls the model complexity
					and hence, helps to control model overfitting
				</Description>
			</Parameters>
			<Parameters>
				<Name>SGD_Data_Fraction</Name>
				<Value>1</Value>
				<Description>Fraction of the training dataset use in a single
					iteration of the optimization algorithm
				</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>DECISION_TREE</Name>
			<Type>Classification</Type>
			<Interpretability>5</Interpretability>
			<Scalability>4</Scalability>
			<Multicollinearity>5</Multicollinearity>
			<Dimensionality>3</Dimensionality>
			<Parameters>
				<Name>Max_Depth</Name>
				<Value>5</Value>
				<Description>Maximum depth of the tree. This helps you to control
					the size of the tree to prevent overfitting.</Description>
			</Parameters>
			<Parameters>
				<Name>Max_Bins</Name>
				<Value>100</Value>
				<Description>Number of bins used when discretizing continuous
					features. This must be at least the maximum number of categories M
					for any categorical feature. Increasing this allows the algorithm
					to consider more split candidates and make fine-grained split
					decisions. However, it also increases computation and
					communication.</Description>
			</Parameters>
			<Parameters>
				<Name>Impurity</Name>
				<Value>gini</Value>
				<Description>The node impurity is a measure of the homogeneity of
					the labels at the node. The current implementation provides two
					impurity measures for classification (gini impurity and entropy)
					and one impurity measure for regression (variance).</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>RANDOM_FOREST</Name>
			<Type>Classification</Type>
			<Interpretability>5</Interpretability>
			<Scalability>4</Scalability>
			<Multicollinearity>5</Multicollinearity>
			<Dimensionality>3</Dimensionality>
			<Parameters>
				<Name>Num_Trees</Name>
				<Value>5</Value>
				<Description>Number of trees in the random forest. Increasing the
					number of trees will decrease the variance in predictions,
					improving the modelâ€™s test-time accuracy. Also training time
					increases roughly linearly in the number of trees.</Description>
			</Parameters>
			<Parameters>
				<Name>Max_Depth</Name>
				<Value>5</Value>
				<Description>Maximum depth of the tree. This helps you to control
					the size of the tree to prevent overfitting.</Description>
			</Parameters>
			<Parameters>
				<Name>Max_Bins</Name>
				<Value>100</Value>
				<Description>Number of bins used when discretizing continuous
					features. This must be at least the maximum number of categories M
					for any categorical feature. Increasing this allows the algorithm
					to consider more split candidates and make fine-grained split
					decisions. However, it also increases computation and
					communication.</Description>
			</Parameters>
			<Parameters>
				<Name>Impurity</Name>
				<Value>gini</Value>
				<Description>The node impurity is a measure of the homogeneity of
					the labels at the node. The current implementation provides two
					impurity measures for classification (gini impurity and entropy)
					and one impurity measure for regression (variance).</Description>
			</Parameters>
			<Parameters>
				<Name>Feature_Subset_Strategy</Name>
				<Value>auto</Value>
				<Description>Number of features to use as candidates for splitting
					at each tree node. The number is specified as a fraction or
					function of the total number of features. Decreasing this number
					will speed up training, but can sometimes impact performance if too
					low.</Description>
			</Parameters>
			<Parameters>
				<Name>Seed</Name>
				<Value>12345</Value>
				<Description>Seed for the random number generator.</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>NAIVE_BAYES</Name>
			<Type>Classification</Type>
			<Interpretability>3</Interpretability>
			<Scalability>5</Scalability>
			<Multicollinearity>1</Multicollinearity>
			<Dimensionality>5</Dimensionality>
			<Parameters>
				<Name>Lambda</Name>
				<Value>1</Value>
				<Description>Additive smoothing can be used by setting this
					parameter.</Description>
			</Parameters>
		</Algorithm>
		<Algorithm>
			<Name>K_MEANS</Name>
			<Type>Clustering</Type>
			<Interpretability>5</Interpretability>
			<Scalability>2</Scalability>
			<Multicollinearity>5</Multicollinearity>
			<Dimensionality>2</Dimensionality>
			<Parameters>
				<Name>Iterations</Name>
				<Value>20</Value>
				<Description>The maximum number of iterations to run.</Description>
			</Parameters>
			<Parameters>
				<Name>Num_Clusters</Name>
				<Value>3</Value>
				<Description>The number of desired clusters.</Description>
			</Parameters>
		</Algorithm>
	</Algorithms>
</MachineLearner>
